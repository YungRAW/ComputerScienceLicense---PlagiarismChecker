Algorithms on Stings, Trees, and Sequences:
Computer Science and Computational Biology
D a n Gusfield
University of CaliÂ¢ornia, Davis
Cambridge University Press, 1998
ISBN: 0-521-58519-8

Preface (Abridged)
The history and motivation
Although I d i d n ' t know it at the time, I began writing this book in the s u m m e r of 1988 when I
was part of a computer science research group at the H u m a n Genome Center of Lawrence Berkeley
Laboratory. Our group followed the standard assumption t h a t biologically meaningful results could
come from considering DNA as a one-dimensional character string, abstracting away th e reality of
DNA as a flexible three-dimensional molecule, interacting in a dynamic environment with protein
and RNA, and repeating a life-cycle in which even the classic linear chromosome exists for only a
fraction of the time. A similar, but stronger, assumption existed for protein, holding for example
t h a t all the information needed for correct three-dimensional folding is contained in the protein
sequence itself, essentiaUy independent of the biological environment the protein lives in. This
assumption has recently been modified, but remains largely intact.
For non-biologlsts, these two assllmptions were (and remain) a god-send allowing rapid entry
into an exciting and i m p o r t a n t field. Statements such as
" T h e digital information t h a t underlies biochemistry, cell biology, an d development
can be represented by a simple string of G's, A's, T's and COs. This string is the root
d a t a structure of an organism's biology."
reinforced the i m p o r t a n c e of sequence-level investigation.
So without worrying much about the more diIBcult chemical an d biological aspects of D N A and
protein, our c o m p u t e r science group was empowered to consider a variety of biologically i m p o r t a n t
problems defined plrimarily on sequences, or (more in the computer science vernacular) on strings.
We organized our efforts into two high-level tasks. First, to learn the relevant biology, laboratory
protocols, and existing algorithmic methods used by biologists. Second to canvass the c o m p u t e r
science literature for ideas and algorithms t h a t weren't already used by biologists, but wkich might
plausibly be of use either in current problems, or in problems t h a t we could anticipate arising w h e n
vast quantities of sequenced DNA or protein become available.

Our p r o b l e m
None of us was an expert on string algorithms. At t h a t point I h a d a t e x t b o o k knowledge of
Knuth-Morris-Pratt~ and a deep confusion about Boyer-Moore (under w h a t circumstances it was
a linear time algorithm, and how to do strong preprocessing in linear time). I u n d e r s t o o d the use
of dyna mi c p r o g r a m m i n g to compute edit distance, b u t otherwise h ad little exposure to specific
string algorithm~ in biology. My general background was in combinatorial optimization, although
I ha d a prior interest in algorithms for building evolutionary trees and h ad studied genetics and
molecular biology in order to pursue that interest.
41

W h a t we needed then, but did.u't have, was a comprehensive cohesive text on string algorithrn~
to guide our education. There were at that time several computer science texts c o n t ~ n i u g a chapter
or two on strings, usuany devoted to a rigorous treatment of Knuth-Morris-Pratt and a cursory
treatment of Boyer-Moore, and possibly an elementary discussion of matching with errors. There
were also some good survey papers that had a somewhat wider scope but didn't treat .their topics
in much depth. There were several texts and edited volumes from the biological side on uses of
computers and algorithms for sequence analysis. Some of these were wonderful in exposing the
potential benefits and the pitfalls of using computers in biology, but generally lacked algorithmic
rigor and covered a narrow range of techniques. Finally, there was the sernlnal text Time Warps,
String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison edited by D.
Sankoff and J. Krnskal, that served as a bridge between algorithms and biology, and had m a n y
applications of dynamic programming. But it too was much narrower t h a n our focus, and a bit
dated.
Moreover, most of the available sources from either cornrnunlty focused on string matching, the
problem of searching for an exact or "nearly exact"- copy of a p attern in a given text. Matching
problems are central, but as detailed in this book, they axe only a part of the m a n y i m p o r t a n t
computational problems defined on strings. So we recognized that s l ~ m e r a need for a rigorous and fundamental treatment of the general topic of algorithmA t h a t operate on strings, along
with a rigorous t r e a t m e n t of specific string algorithm~ of greatest current and potential import in
computations] biology. This book is an attempt to provide such a dual, and integrated, t r e a t m e n t .
Why mix Computer

Science a n d C o m p u t a t i o n a l Biology in one book?

My interest in computational biology began in 1980, when I started reading papers on bu.i]ding
evolutionary trees. At that point, computational molecular biology was a largely undiscovered area
for computer science, although it was an active area for statisticians and mathematicians (notably
Michael W a t e r m a n and David S~n~off who have largely framed the field). But seventeen years
later, computational biology is hot, and many computer scientists are now entering the (now more
hectic, more competitive) field. W h a t should they learn.7
The problem is t h a t the emerging field of computational molecular biology is not wen defined and its definition is m a d e more difficult by rapid changes in m o l e c u l ~ biology itself. Still,
algorithrn~ that operate on molecular sequence data (strings) are at the heart of computational
molecular biology. The big-picture question in computational molecular biology" is how to "do" as
much "real biology" as possible by exploiting molecular sequence data (DNA, RN A and protein).
Getting sequence data is relatively cheap and fast (and getting more so) compared to more traditional laboratory investigations. The use of sequence data is already central in several subareas of
molecular biology and the full impact of having extensive sequence d a t a is yet to be seen. Hence,
algorithms that operate on strings will continue to be the area of closest intersection and interaction
between computer science and molecular biology. Certainly then, computer scientists need to learn
the string techniques that have been most successfully applied. But that is not enough.
Computer scientists need to learn h m d a m e n t a l ideas and techniques t h a t will endure long
after today's central motivating applications are forgotten. They need to study meth od s t h a t
prepare t h e m to frame and tackle future problems and applications. Signi6cant contributions to
computational biology might be made by extending or adapting algorithm~ from computer science,
even when the original algorithm has no clear utility in biology. Therefore, the computer scientist
who wants to enter t h e general field of computational molecular biology and. who learns string
algorithms with that end in mind, should receive a training in string algorithms t h a t is much
broader t h a n a tour through techniques of known present application. So even ff I were to write

42

a book for computer scientists who only want to do computational biology, I would still choose to
include a broad range of algorithmic techniques from pure computer science.
In this book, I cover a wide spectrnrn of string techniques, well beyond those of established
utility, but I select ~ o m the m a n y possible illustrations, those techniques t h a t seem to have the
greatest potential application in future molecular biology. Potential application, particularly of
ideas rather t h a n of concrete methods, and to anticipated rather t h a n to existing problems, is a
m a t t e r of j u d g m e n t and speculation. No doubt, some of the material contained in this book will
never find direct application in biology, while other material will find uses in surprising ways.
Fonowing the above discussion, this book is a general-purpose rigorous t r e a t m e n t of the entire
field of deterministic algorithms t h a t operate on strings and sequences. M a n y of those algorithms
utilize trees as data- structures, or arise in biological problems related to evolutionary trees, hence
the inclusion of "trees" in the title.
The model reader is a research-level professional in computer science or a graduate or advanced
u n d e r g r a d u a t e student in computer scienee~ although there are m a n y biologists (and of course
mathematici~nR) with s11~cient algorithmic background to read the book. The book is intended
to be b o t h a reference, and a main text for courses in pure computer science, and for computer
science oriented courses on computational biology.
Explicit discussions of biological applications appear throughout the book, but are more concentrated in the last sections of P a r t II, and in most of Parts III and IV. I discuss a nlwnber of
biological issues in detail in order to give the reader a deeper appreciation for the reasons t h a t
m a n y biological problems have been cast as problems on strings, and for the variety of (often very
imaginative) technical ways t h a t string algorithms have been employed in molecular biology.
This book covers all the classic topics and most of the important advanced techniques in the
field of string algorithm% with three exceptions. It only lightly touches on probabilistic analysis,
does not discuss parallel algorithms, or the elegant, but very theoretical results on algorithms for
infinite alphabets and on algorithms using only constant au.x.iliary space. The book also does not
cover stochastic oriented methods t h a t have come out of the machine learniug comm,mlty, although
some of the algorithms in this book are extensively used as subtools in those methods. W i t h these
exceptions, the book covers all the major styles of thinlcing about string algorithms. The reader
who absorbs the material in this book will gain a deep and broad understanSing of the field, and
s~Wicient sophistication to undertake original research.
Reflecting my background, the book rigorously discusses each of the topics, usually providing
complete proofs of behavior (correctness~ worst-case time and space). More important, it emphasizes the ideas and derivations of the methods it presents, rather t h a n simply providing an inventory
of available algorithms. To better expose ideas and encourage discovery, I often present a complex algorithm by introducing a naive, inefficient, version and t h e n successively applying additional
insight and implementation detail to obtalu the desired result.
The book contains some new approaches I developed to explain certain classic and complex
material. In particular, the preprocessing methods I present for Knuth-Morris-Pratt, Boyer-Moore
and several other linear-time p a t t e r n matching algorithm% differ from the classical methods, b o t h
imiFying and simplifying the preprocessing tasks needed for those algorithms. I also expect t h a t
my (hopefully simpler and clearer) expositions on linear time s ~ x tree constructions and on
the constant time ]east common ancestor algorithm will make those i m p o r t a n t methods more
available and widely understood. I connect theoretical results from computer science on sublineartime algorithms, with widely used methods for biological database search. In the discussion of
multiple sequence alignment, I bring together the three m a j o r objective functions t h a t have been
proposed for multiple alignment, and show a continuity between approyimation algorithrn.q for

43

those three multiple alignment problems. Similarly, the chapter on evolutionary tree construction
exposes the commonality of several distinct problems and solutions in a way t h a t is not well known.
Throughout the book, I discuss m a n y computational problems concerning repeated substrings
(a very widespread phenomenon in DNA). I consider several different ways to define repeated
substrings and use each specific definition to explore computational problems and algorithms on
repeated substrings.
In the book I t r y to explain in complete detail, and at a reasonable pace, m a n y complex methods
that have previously been written exclusively for the specialist in string algorithrn~. I avoid detailed
code, as I find it rarely serves to exp]~in interesting ideas, and I provide over 400 exercises to both
reinforce the material of the book, and to develop additional topics.

Table of Contents
Preface
The history and motivation / W h y mix Computer Science and Computational Biology
in one book? / W h a t the book is / W h a t the book is not / Acknowledgements

I EXACT

STRING

MATCHING

The Fundamental String Problem / Exact matching: W h a t ' s the problem? / Importance of the exact mat~.hi=g problem / Overview of Part I / Basic string definitions

C h a p t e r 1. E x a c t M a t c h i n g : F u n d a m e n t a l P r e p r o c e s s i n g a n d First A l g o r i t h m s
1.1 The Naive method / 1.1.1 Early ideas for speeding up the naive method / 1.2
The preprocessing approach / 1.3 Fundamental preprocessing of the pattern / 1.4
Fundamental preprocessing in linear time / 1.5 The simplest linear-time exact matching
algorithm / 1.5.1 W h y continue? / 1.6 Exercises /

C h a p t e r 2 E x a c t M a t c h i n g : Classical C o m p a r i s o n - B a s e d M e t h o d s
2.1 Introduction / 2.2 The Boyer-Moore Algorithm / 2.2.1 Right to left scan / 2.2.2
Bad character rule / 2.2.2.1 Extended bad character rule / 2.2.2.2 Implementing the
extended bad character rule / 2.2.3 The (strong) good s11Â¢B~rule / 2.2.4 Preprocessing
for the good s11~x rule / 2.2.5 The good s11fflx rule in the search stage of Boyer-Moore
/ 2.2.6 The complete Boyer-Moore algorithm / 2.3 The Knuth-Morris-Pratt algorithm
/ 2.3.1 The Knuth-Morris-Pratt shift idea / 2.3.1.1 The Knuth-Morris-Pratt shift rule
/ 2.3.2 Preprocessing for Knuth-Morris-Pratt / 2.3.3 A full implementation of KnuthMorris-Pratt / 2.4 Real-time string matching / 2.4.1 Converting Knuth-Morris-Pratt to
a real-time method / 2.4.2 Preprocessing for real-time string matching / 2 . 5 Exercises
C h a p t e r 3 E x a c t M a t c h i n g : A d e e p e r look at classical m e t h o d s
3.1 A Boyer-Moore variant with a "simple" linear time bound / 3.1.1 Key ideas /
3.1.2 One phase in detail / 3.1.2.1 Phase algorithm / 3.1.3 Correctness and linear time
analysis / 3.2 Cole's linear worst case bound for Boyer-Moore / 3.2.1 Cole's proof when
the pattern does not occur in the text / 3.2.1.1 An initial lemma / 3.2.1.2 Return to
44

Cole's proof / 3.2.2 The case when the p a t t e r n does occur in the text / 3.2.3 Adding in
the bad character rule / 3.3 The original preprocessing for Knuth-Morris-Pratt / 3.3.1
The m e t h o d does not use fundamental preprocessing / 3.3.2 The easy case / 3.3.3 The
general case / 3.3.4 The complete preprocessing algorithm / 3.3.5 How to compute the
optimized shift values / 3.4 Exact matching with a set of patterns / 3.4,1 Naive use
of keyword trees for set matching / 3.4.2 The speed-up: generalizing Knuth-MorrisP r a t t / 3.4.3 Failure functions for the keyword tree / 3.4.4 The failure links speed up
the search / 3.4.5 Linear preprocessing for the failure function / 3.4.6 The full AhoCorasick algorithm: relaxing the substring assnmption / 3.4.6.1 Tmplementation / 3.5
Three applications of exact set matching / 3.5.1 Matching against a DNA or protein
library of known patterns / 3.5.2 Exact matching with wild cards / 3.5.2.1 Correctness
and complexity of the method / 3.5.3 Two dimensional exact matching / 3.6 Regular
Expression P a t t e r n Matching / 3.6.1 Formal Definitions / 3.7 Exercises
Chapter 4 Semi-Numerical

String Matching

4.1 Arithmetic verses comparisons / 4.2 The Shift-And method / 4.2.1 H o w to construct
array M / 4.2.2 Shlft-And is effective for small patterns / 4.2.3 agrep: the Shift-And
method with errors / 4.2.4 H o w to compute M_k / 4.3 The match-count problem and
Fast Fourier Transform / 4.3.1 A fast worst-case method for the match-count problem?
/ 4.3.2 Using Fast Fourier Transform for match-counts / 4.3.2.1 The high-level approach
/ 4.3.2.2 Cyclic correlation / 4.3.2.3 Handllng wildcaxds in match-counts / 4.4 KarpRabin fingerprint methods for exact match / 4.4.1 Arithmetic replaces comparisons /
4.4.2 Fingerprints of P and T / 4.4.2.1 Prime moduli limit false matches / 4.4.2.2 The
Central Theorem / 4.4.2.3 Extensions / 4.4.2.4 Even lower limits on error / 4.4.2.5
Checking for error in linear time / 4.4.3 W h y fingerprints? / 4.5 Exercises

PART

II Suffix trees

Chapter 5 Introduction

and their uses

to suffix trees

5.1 A short history / 5.2 Basic definitions / 5.3 A motivating example / 5.4 A naive
algorithm to build a sllf~x tree
Chapter 6 Linear time construction

of suffix trees

6.1 Ukkonen's linear time su.i~.x tree algorithm / 6.1.1 Implicit suffix trees / 6.1.2
Ukkonen's algorithm at a high level 6.1.3 Implementation and speedup / 6.1.3.1 S11mx
].inks: first implementation speedup / 6.1.3.2 Following a trail of suffix links to build
I ( i + l ) / 6.1.3.3 Single extension algorithm: SEA / 6.1.3.4 Trick nl,mber 1: skip/count
trick / 6.1.4 A simple implementation detail / 6.1.5 Two more little tricks and we're
done / 6.1.5.1 The punch line / 6.1.6 Creating the true s11~x tree / 6.2 Weiner's linear
time sufFLX tree algorithm / 6.2.1 A straightforward construction / 6.2.2 Towards a
more efficient implementation / 6.2.2.1 Finding Head(i) efficiently / 6.2.3 The basic
idea of Weiner's algorithm / 6.2.3.1 The algorithm in the good case / 6.2.3.2 The two
degenerate cases / 6.2.4 The Full Algorithm for creating T(i) from T(i-t-1) / 6.2.4.1
Correctness / 6.2.4.2 How to update the vectors / 6.2.5 Time analysis of Weiner's
algorithm / 6.2.6 Last comments about Weiner's algorithm / 6.3 McCreight's su~.x tree

45

algorithm / 6.4 Generalized suffix tree for a set of strings / 6.5 Practical implementation
issues / 6.5.1 Alphabet independence: all linears are equal, but some are more equal
t h a n others / 6.6 Exercises
Chapter

7 F i r s t a p p l i c a t i o n s o f suffix t r e e s

7.1 A P L I : Exact string matching / 7.2 APL2: SllfB~ trees and the exact set matching
problem / 7.2.1 Comparing s,d~i~ trees and key-word trees for exact set matching / 7.3
APL3: The substring problem for a database of patterns / 7.4 APL4: Longest common
substring of two strings / 7.5 APLS: Recognizing DNA cont~mlnation / 7.6 APL6:
Common substrings of more t h a n two strings / 7.6.1 Computing the C(v) n~lmbers /
7.7 APL7: Building a smaller directed graph for exact m~tchi~g / 7.8 APL8: A reverse
role for s-rex trees, and major space reduction / 7.8.1 Matching statistics: duplicating
bounds and reducing space / 7.8.2 Correctness and time analysis for matching statistics
/ 7.8.3 A small but important extension / 7.9 APL9: Space etilcient longest common
substring / 7.10 APL10: All-Pairs S,lSx-Prefix Matching / 7.10.1 Solving the all-pairs
smTLx-prefLx problem in linear time / 7.11 Introduction to repetitive structures in strings
/ 7.11.1 Repetitive structures in biological strings / 7.11.2 Uses of repetitive structures
in molecular biology / 7.12 APL11: Finding all maximal repetitive structures in linear
time / 7.12.1 A linear time algorithm to find all ma0~irnal repeats / 7.12.2 Finding
supermaximal repeats in linear time / 7.12.3 Finding all the ma~.imal pairs in linear
time / 7.13 APL12: Circular string linearization / 7.13.1 Solution via svmx trees /
7.14 APL13: S - ~ x Arrays: more space reduction / 7.14.1 S~lmx tree to s~mx array in
linear time / 7.14.2 How to search for a pattern using a s , ~ x array / 7.14.3 A simple
accelerant / 7.14.4 A super-accelerant / 7.14.5 How to obtain the Lcp values / 7.14.6
Where do large alphabet problems arise? / 7.15 APL14: S!lmx trees in genome-scale
projects / 7.16 APL15: A Boyer-Moore approach to exact set matching / 7.16.1 The
search / 7.16.2 Bad character rule / 7.16.3 Good s , m x rule / 7.16.4 How to deterr~i~e i2
and i3 / 7.16.5 An implementation e]imlnating redundancy / 7.17 APL16: Ziv-Lempel
data compression / 7.17.1 Implementation using s,~mx trees / 7.17.2 A one-pass version
/ 7.17.3 The real Ziv-Lempel / 7.18 APL17: M~nim~]m length encoding of DNA / 7.19
Additional applications / 7.20 Exercises
Chapter 8 Constant time lowest common ancestor retrieval
8.1 Introduction / 8.1.1 W h a t do ancestors have to do with strings? / 8.2 The sssllmed
machine model / 8.3 Complete binary trees: A very simple case / 8.4 How to solve
lca queries in B / 8.5 First steps in mapping T to B / 8.6 The mapping of T to B /
8.7 The linear time preprocessing of T / 8.8 Answering an lca query in constant time
/ 8.9 The binary tree is only conceptual / 8.10 For the purists: how to avoid bit-level
operations / 8.11 Exercises
C h a p t e r 9 M o r e a p p l i c a t i o n s of suffix trees
9.1 Longest common extension: a bridge to inexact matching / 9.1.1 Linear time
solution / 9.1.2 Space ei[icient longest common extension / 9.2 Finding all maximal
palindromes in linear time / 9.2.1 Linear time solution / 9.2.2 Complemented and

46

separated palindromes / 9.3 Exact matching with wild cards / 9.4 The k-mismatch
problem / 9.4.1 The solution / 9.5 Appro~rlmate palindromes and repeats / 9.6 Faster
methods for tandem repeats / 9.6.1 The speedup for k-mismatch tandem repeats / 9.7
A linear time solution to the multiple common substring problem / 9.7.1 The method
/ 9.7.2 Time analysis / 9.7.3 Related uses / 9.8 Exercises

P A R T III Inexact Matching, Sequence Alignment, and D y n a m i c
Programming
C h a p t e r 10 T h e i m p o r t a n c e of ( s u b ) s e q u e n c e c o m p a r i s o n in m o l e c u l a r biology
C h a p t e r 11 C o r e s t r i n g edits, a l i g n m e n t s And d y n a m i c p r o g r s r n r n i n g
11.1 Introduction / 11.2 The edit distance between two strings / 11.2.1 String alignment / 11.3 Dynamic programming calculation of edit distance / 11.3.1 The recurrence
relation / 11.3.2 Tabular computation of edit distance / 11.3.3 The traceback / 11.3.3.1
The pointers represent all optimal edit transcripts / 11.4 Edit graphs / 11.5 Weighted
edit distance / 11.5.1 Operation weights / 11.5.2 Alphabet-weight edit distance / 11.6
String similarity / 11.6.1 Computing similarity / 11.6.2 Special cases of similarity /
11.6.3 Alignment graphs for similarity / 11.6.4 Endspace free variant / 11.6.5 Approximate occurrences of P in T / 11.7 Local alignment: Finding substrings of t~.igh
similarity / 11.7.1 Computing local alignment / 11.7.2 How to solve the local s-flex
alignment problem / 11.7.3 Three final comments on local alignment / 11.8 .Gaps /
11.8.1 Introduction to Gaps / 11.8.2 Why gaps? / 11.8.3 cDNA matching: a concrete
illustration / 11.8.3.1 Processed Pseudogenes / 11.8.4 Choices for gap weights / 11.8.5
Arbitrary gap weights / 11.8.5.1 Time analysis / 11.8.6 Arlene (and constant) Gap
Weights / 11.8.6.1 The recurrences / 11.8.6.2 Time analysis / 11.9 Exercises

Chapter 12 Refining Core String Edits and Alignments
12.1 Computing alignments in only linear space / 12.1.1 Space reduction for computing
similarity / 12.1.2 How to find the optimal alignment in linear space / 12.1.3 The full
idea: Use recursion / 12.1.4 Time analysis / 12.1.5 Extension to local alignment /
12.2 Faster algorithms when the nunlber of differences is bounded / 12.2.1 Where do
bounded difference problems arise? / 12.2.2 Illustrations from molecular biology /
12.2.3 k-di~erence global alignment / 12.2.4 The return of the s11~x tree: k-difference
inexact matching / 12.2.5 The primer (and probe) selection problem revisited / 12.2.5.1
How to solve the k-difference primer problem / 12.3 Exclusion methods: fast expected
r , nniug time / 12.3.1 The BYP method / 12.3.2 Expected time analysis of Algorithm
BYP / 12.3.3 The Chang-Lawler method / 12.3.4 Multiple filtration for k-mismatches
/ 12.3.5 Myers' sublinear-time method / 12.3.6 Final comment on exclusion methods /
12.4 Yet more sufF~-trees, more hybrid dynamic progrR.mmlug / 12.4.1 The P-againstall problem / 12.4.2 The (threshold) all-against-all problem / 12.4.2.1 Correctness and
time analysis / 12.5 A faster (combinatorial) algorithm for longest common suhsequence
/ 12.5.1 Longest increasing subsequence / 12.5.2 Longest common subsequence reduces
to longest increasing sequence / 12.5.3 How good is the method / 12.5.4 The lcs of more
than two strings / 12.6 Convex Gap weights / 12.6.1 Forward dynamic programming
/ 12.6.2 The basis of the speedup / 12.6.3 Cell pointers and row partition / 12.6.3.1
4?

Preparation for the speedup / 12.6.4 Final implementation details and time analysis /
12.6.4.1 The case of F values is essentia].ly symmetric / 12.7 The Four-Russians speedup
/ 12.7.1 t-blocks / 12.7.2 The Four-Russians idea for the restricted block function /
12.7.2.1 Accounting detail / 12.7.3 The trick: offset encoding / 12.7.3.1 Time analysis
/ 12.7.4 Practical approaches / 12.8 Exercises
C h a p t e r 13 E x t e n d i n g t h e core p r o b l e m s
13.1 Parametric Sequence Alignment / 13.1.1 Introduction / 13.1.2 Definitions and
first results / 13.1.3 Parametric alignment with the use of scoring matrices / 13.1.4
Et~cient algorithms for computing a polygonal decomposition / 13.1.4.1 Finding a
polygon of the decomposition / 13.1.4.2 Filling in the parameter space / 13.1.5 Time
analysis and next idea / 13.1.6 Bounding the n~lmber of polygons in the decomposition
/ 13.1.6.1 The special case of global alignment / 13.1.7 Uses for parametric alignment
/ 13.1.7.1 Sensitivity analysis / 13.1.7.2 Et~cient computation of all co-optimals / 13.2
Computing suboptimal alignments / 13.2.1 First definitions and first results / 13.2.2 A
useful re-weighting / 13.2.3 Counting and enumerating near-optimal paths / 13.2.4 An
alternative approach to suboptimal alignment / 13.3 Cb~i~i~g diverse local alignments
/ 13.4 Exercises
C h a p t e r 14 M u l t i p l e S t r i n g C o m p a r i s o n - T h e H o l y G r a i l
14.1 Why multiple string comparison? / 14.1.1 Biological basis for multiple string comparison / 14.2 Three "big-picture" biological uses for multiple string comparison / 14.3
Family and superfarnily representation / 14.3.1 Family representations and alignments
with profiles / 14.3.1.1 Aligning a string to a profile / 14.3.2 Signature representstions of families / 14.3.2.1 Signatures for Helicase proteins / 14.4 Multiple sequence
comparison for structural inference / 14.5 Introduction to computing multiple string
alignments / 14.5.1 How to score multiple alignments / 14.6 Multiple alignment with
the s11m-of-pairs (SP) objective function / 14.6.1 An exact solution to the SP a].ignment problem / 14.6.1.1 A speed up for the exact solution / 14.6.2 A bounded-error
approximation method for SP alignment / 14.6.2:1 An initial key idea: Alignments
consistent with a tree / 14.6.2.2 The center star method for SP alignment / 14.6.3
Weighted SP alignment / 14.7 Multiple alignment with consensus objective functions
/ 14.7.1 Steiner consensus strings / 14.7.2 Consensus strings from multiple alignment /
14.7.3 Approximating the optimal consensus multiple alignment / 14.8 Multiple alignment to a (phylogenetic) tree / 14.8.1 A heuristic for phylogenetic alignment / 14.8.1.1
The error analysis / 14.8.1.2 Computing the minimllm distance lifted alignment /
14.9 Comments on bounded-error approyimations / 14.10 Common multiple alignment
methods / 14.10.1 Iterative paixwise alignment / 14.10.2 Two specific illustrations of
iterative pairwise alignment / 14.10.2.1 Iterative multiple alignment to identify protein
secondary structure / 14.10.2.2 Iterative multiple alignment to build evolutionary trees
/ 14.10.3 Repeated-motif methods / 14.10.4 Two newer approaches to multiple string
comparison / 14.11 Exercises
C h a p t e r 15 S e q u e n c e D a t a b a s e a n d t h e i r uses - t h e M o t h e r L o d e

48

15.1 Success stories of database search / 15.1.1 The first success story / 15.1.2 A more
recent example of successful database search / 15.1.3 Indirect applications of database
search / 15.2 The database industry / 15.3 Algorithmic issues in database search /
15.3.1 Should there be any? / 15.4 Real Sequence Database search / 15.5 FASTA /
15.6 BLAST / 15.6.1 The hit (hot-spot) strategy of BLAST / 15.6.2 The effectiveness
of BLAST / 15.7 PAM: the first major e.rniuo acid substitution matrices / 15.7.1 PAM
11nlts and PAM matrices / 15.7.2 PAM units / 15.7.3 PAM matrices / 15.7.4 How are
PAM matrices actually derived? / 15.7.5 The use of the PAM matrix / 15.8 P R O S I T E
/ 15.9 BLOCKS and BLOSUM / 15.10 The BLOSUM substitution matrices / 15.11
Additional considerations for database searchlng / 15.11.1 Statistical significance /
15.11.2 A theory of log-odds scores / 15.11.3 Importance of searching protein with
protein / 15.12 Exercises

P A R T IV Currents, Cousins and Cameos
Chapter 16 Maps, Mapping, Sequencing And Superstrings
16.1 A look at some DNA mapping and sequencing problems / 16.2 Mapping and the
genome project / 16.3 Physical versus genetic maps / 16.4 Physical mapping / 16.5
Physical mapping: STS-content mapping and ordered clone libraries / 16.5.1 Reconstruction of STS order / 16.6 Physical mapping: Radiation-hybrid mapping / 16.6.1
Reconstruction of STS order in radiation hybrids / .16.6.2 Traveling salesman formulation of STS ordering / 16.6.3 Back to STS-content mapping: the case of errors / 16.7
Physical mapping: Fingerprinting for general map construction / 16.8 Computing the
tightest layout / 16.9 Physical mapping: last comments / 16.10 An introduction to
map alignment / 16.10.1 A non-unary dynamic programming approach to map alignment / 16.10.2 Extensions of the map alignment model / 16.11 Large-scale sequencing
and sequence assembly / 16.12 Directed sequencing / 16.13 Top-down, bottom-up sequencing: The picture using YACs / 16.13.1 Is mapping necessary for sequencing? /
16.13.2 Fragment selection for sequencing / 16.13.3 Some real numbers / 16.14 Shotgun
DNA sequencing / 16.15 Sequence assembly / 16.15.1 Step one: Overlap detection /
16.15.2 Step two: string layout / 16.15.3 Step three: Deciding the consensus / 16.16
Final comments on top-down, bottom-up sequencing 7 16.17 The shortest superstring
problem / 16.17.1 Basic Definitions / 16.17.2 The objective function for superstrings /
16.17.3 Cyclic strings and cycle covers / 16.17.4 How cycle covers define superstrings
/ 16.17.5 Factor-of-four approximation / 16.17.5.1Error analysis of the algorithm /
16.17.6 Improvement to a factor of three / 16.17.6.1 Error analysis / 16.17.7 Efficient
implementation / 16.17.7.1 Non-trivial cycle cover / 16.16.7.2 How to form the matrix efficiently / 16.18 Sequencing by hybridization / 16.18.1 Reduction to Euler paths
/ 16.18.2 Continuity of compatible strings / 16.18.3 Last comments on SBH / 16.19
Exercises
C h a p t e r 17 S t r i n g s a n d E v o l u t i o n a r y T r e e s
17.1 Ultrametric trees and ultrametric distances / 17.1.1 Introduction / 17.1.2 Evolutionary trees as ultrametric trees / 17.1.3 How to test for an ultrametric tree / 17.1.4
How are ultrametric data obtained? / 17.1.4.1 Laboratory-based methods / 17.1.4.2
Sequence-based methods / 17.1.4.3 Final comments / 17.2 Additive-distance trees /
49

17.2.1 Introduction / 17.2.2 Algorithms for the additive tree problem / 17.2.2.1 Compact additive trees / 17.3 Parsimony: character-based evolutionary" reconstruction /
17.3.1 Introduction / 17.3.2 Where do character data come fi-om? / 17.3.3 Perfect
Phylogeny / 17.3.4 An O(=m)-time algorithm for the perfect phylogeny problem /
17.3.5 Tree compatibility: an application of perfect phylogeny / 17.3.6 Generalized
perfect phylogeny / 17.4 The centrality of the ultrametric problem / 17.4.1 The additive tree problem viewed as an ultrametric problem / 17.4.2 The perfect phylogeny
problem viewed as an ultrametric problem / 17.5 Ma~im,lm parsimony, Steiner trees
and Perfect Phylogeny / 17.5.1 Basic definitions / 17.5.2 Approyimations to ma~rirnllrn
parsimony / 17.6 Phylogenetic Align ment, again / 17.6.1 The Fitch-Hartigan min irnnm
mutation problem / 17.6.2 Phylogenetic alignment used to compute PAM matrices /
17.7 Connections between multiple alignment and tree construction / 17.8 Exercises
C h a p t e r 18 T h r e e s h o r t t o p i c s
18.1 Matching DNA to protein with ~arneshift errors / 18.1.1 Matching a string to
a network / 18.1.2 DNA/protein matching cast as network matching / 18.2 Gene
prediction / 18.2.1 Exon assembly / 18.3 Molecular computation: Computing with
(not about) DNA strings / 18.3.1 Lipton's approach to the Satist]ability Problem /
18.3.2 Critique / 18.4 Exercises
C h a p t e r 19 M o d e l s o f g e n o m e - l e v e l m u t a t i o n s
19.1 Introduction / 19.1.1 Genome rearrangements give new evolutionary insights /
19.2 Genome rearrangements with inversions / 19.2.1 Definitions and initial facts /
19.2.2 The heuristics / 19.2.2.1 Tmproving the guaxemtee / 19.3 Signed inversions /
19.4 Exercises
C h a p t e r 20 E p i l o g u e - W h e r e N e x t ?
Chapter 21 Glossary
Index

Complexity Theory Retrospective II
Lane A. Hemaspaandra and Alan L. Sel~a~a, editors
Springer-Verlag, New York, 1997
ISBN 0-387-94973-9

http://~.springer-ny.com/catalog/np/max97np/DATA/O-387-94973-9.htL1
Blurb

(in Lieu

of the

Preface)

Complexity theory is a flourishing area of research that continues to provide one of the richest
sources of research problemR in computer science. This volllrne, a collection of articles written by
50

experts, provides a survey of the subject, a comprehensive guide to research, and a provocative
look to the future.
The editors' aim has been to provide an accessible description of the current state of complexity
theory and to demonstrate the breadth of techniques and results that make the subject exciting.
Papers are on traditional topics ranging from sublogarithmlc space to exponential time, on new
combinatorial techniques and recent successes such as interactive proof systems, and on the newly
emerging areas of q u a n t u m and biological computing. As a result, researchers and students in
computer science will find this book an excellent starting point for study of the subject and a
useful source of the key known results.

Table of Contents
Preface

1 T i m e , H a r d w a r e , a n d U n i f o r m i t y - - David Mix Barrington, Nell I m m e r m a n

1.1 Introduction / 1.2 Background: Descriptive Complexity / 1.3 First Uniformity
Theorem / 1.4 Variables That Are Longer Than log(n) Bits / 1.5 Uniformity: The
Third Dimension / 1.6 Variables That Are Shorter Than log(n) Bits / 1.7 Conclusions
2 Q u a n t u m C o m p u t a t i o n - - Andre Berthia~ime

2.1 The Need for Quantum Mechanics / 2.2 Basic Principles of Quantum Mech~m~cs
/ 2.2.1 Probability Amplitudes / 2.2.2 Qubits and H o w to Observe T h e m / 2.2.3 Digression on Quantum Cryptography / 2.2.4 Evolution of a Quantum System / 2.2.5
Quant1~rn Registers / 2.3 Computing with Quantum Registers / 2.4 Separating T w o
Classes of Functions / 2.5 Shor's FactoRing Algorithm / 2.6 Building a Quant~m Computer
3 S p a r s e S e t s v e r s u s C o m p l e x i t y C l a s s e s - - Jin-Yi Cai and Mitsunori Ogihara

3.1 Introduction / 3.2 Earlier Results for Tu.ring Reductions / 3.2.1 Sparse Sets and
Polynomial Size Circuits/ 3.2.2 The Karp-Lipton Theorem / 3.2.3 Long's Extension
/ 3.3 Earlier Results for Many-One Reductions / 3.3.1 The Isomorphism Conjecture
for N P / 3.3.2 Mahaney's Theorem / 3.4 Bounded Truth Table Reduction of N P /
3.4.1 Extensions / 3.5 The Hartmanis Conjecture for P / 3.5.10gihara's Language
and Randomized N C 2 / 3.5.2 Deterministic Construction / 3.5.3 The Finale: N C 1
Simulation / 3.6 Conclusions
4 C o u n t i n g C o m p l e x i t y - - Lance Fortnow

4.1 Introduction / 4.2 Preliminaries / 4.3 Counting Functions / 4.3.1 Algebraic Properties of Counting Functions / 4.3.2 A Randomized sign Function / 4.3.3 Counting
Functions and the Polynomial-Time Hierarchy / 4.4 Counting Classes / 4.4.1 Classifying Counting Classes / 4.2 Counting Operators / 4.3 The Polynornial-Time Hierarchy
/ 4.4 Closure Properties of P P / 4.5 Relativization / 4.6 Other Work / 4.6.1 Circuits / 4.6.2 Lowness / 4.6.3 Characterizing Specific Problems / 4.6.4 Interactive Proof
Systems / 4.6.5 Counting in Space Classes / 4.6.6 Other Research

51

5 A T a x o n o m y o f P r o o f S y s t e m s ~ Oded Goldreich
5.1 Introduction / 5.2 A Technical Exposition / 5.2.1 Interactive Proof Systems / 5.2.2
MIP and P C P / 5.2.3 Computationally Sound Proof Systems / 5.2.4 Other Types of
Proof Systems / 5.2,5 Comparison / 5.3 The Story / 5.3.1 The Evolution of Proof SysteIns / 5.3.2 P C P and Approximation / 5.3.3 Interactive Proofs and Program Checking
/ 5.3.4 Zero-Knowledge Proofs
6 S t r u c t u r a l P r o p e r t i e s o f C o m p l e t e P r o b l e m s for E x p o n e n t i a l T i m e

~ Steven Homer

6.1 Introduction / 6.2 Strong Reductions to Complete Sets / 6.3 Tmrmmlty for Complete Problems / 6.4 Differences between Complete Sets / 6.5 Other Properties and
Open Problems / 6.5.1 Properties of "Weak" Complete Sets / 6.5.2 Polynomial-Time
Complete Recursively Enumerable Sets / 6.5.3 A Short List of Open Problems
7' T h e C o m p l e x i t y o f O b t a i n i n g S o l u t i o n s for P r o b l e m s i n N P n - d N L - - Birgit Jenner
and Jacobo Toran
7.1 Introduction / 7.2 Computing Optimal Solutions: The Class F P N P / 7.3 Bounded
Queries to NP / 7.4 Computing Solutions Uniquely: The Class N P S V / 7.5 Nonadaptire Queries to 1NP: The Class P p t t N p / 77.6 A Look inside Nondeterministic Logspace
/ 7.7' Conclusions

8 Biological Computing
Simon

- - Stuart A. Kurtz, Stephen R. Mahaney, James S- Royer, and Janos

8.1 Introduction / 8.2 The One-Molecule Processor / 8.3 A Brief Introduction to Biochemistry / 8.3.1 DNA, RNA, and Proteins / 8.3.2 Protein Synthesis / 8.4 Computational Molecules / 8.4.1 CNA / 8 . 4 . 2 t C N A / 8.4.3 The Synthesis of t C N A / 8.5 The
Microarchitectttre of CNA Computers / 8.6 A Brief Discussion of Adleman's Model
Versus Our Model / 8.7 Conclusions
9 Computing

w i t h S u b l o g a r i t b m i c S p a c e - - Maciej Liskiewicz and Ruediger Reischuk

9.1 Are Sublogarithmlc Space Classes of Any Interest? / 9.2 The Alternating Sublogarithmic Space World / 9.3 Adding Randomness / 9.4 Special LJmitatious of Machines
with a Sublogarithmic Space Bound / 9.4.1 Technical Preliminaries / 9.4.2 Inputs with
a Periodic Structure / 9.4.3 Fooling A T M s / 9.5 A Survey of Lower Space Bound Proofs
/ 9.5.1 Languages for Separating the Levels of the Alternation Hierarchy / 9.5.2 A T M s
with a Constant N11mber of Alternations / 9.5.3 Unbounded Alternation / 9.5.4 Closure Properties / 9.5.5 Lower Bounds for Context-Free Languages / 9.6 Conclusions
and Open Problems
10 T h e Q u a n t i t a t i v e

S t r u c t u r e o f E x p o n e n t i a l T i m e - - Jack H. Lutz

52

10.1 Introduction / 10.2 Prellmlnaries / 10.3 Resource-Bounded Measure / 10.4 Incompressibilityand Bi-Immunity / 10.5 Complexity Cores / 10.6 Small Span Theorems /
I0.7 Weakly Hard Problems / 10.8 Upper Bounds for Hard Problems / 10.9 Nonuniform
Complexity, Natural Proofs, and Pseudorandom Generators / 10.10 W e a k Stochasticity / 10.11 Density of Hard Languages / 10.12 Strong Hypotheses / 10.13 Conclusions
and Open Directions
11 P o l y n o m i a l s

and Combinatorial

Definitions

of Ln~guages

-- Kenneth W. Regan

II.I Introduction / 11.2 Polynomials / 11.3 Representation Schemes and Language
Classes / 11.4 Strong versus W e a k Representation / 11.5 K n o w n Upper and Lower
Bounds on Degree / 11.6 Polynomials for Closure Properties / 11.7 ProbabillsticPolynomials / 11.8 Other Combinatorial Structures /
12 Average-Case C o m p u t a t i o n a l C o m p l e x i t y T h e o r y -

Jie Wang

12.1 Introduction / 12.2 Average Polynomial Time / 12.3 Average-Case Completeness
/ 12.3.1 Polynomial-Time Reductions / 12.3.2 Polynomial-Time Computable Distributions / 12.3.3 Uniform Distributions / 12.3.4 Distribution Controlling Ler~ma /
12.3.5 Distributional NP-Completeness / 12.3.6 Average Polynomial-Time Reductions
/ 12.3.7 Distributional Search Problems / 12.4 Randomization / 12.4.1 Flat Distributions and Incompleteness / 12.4.2 Randomized Average Polynomial Time / 12.4.3 Randomizing Reductions and Completeness / 12.4.4 Polynomial-Time Sampling / 12.4.5
Randomized Turing Reductions / 12.5 Hierarchies of Average-Case Complexity / 12.5.1
Average-Time Hierarchies / 12.5.2 Fast Convergence of Average Time / 12.5.3 Averaging on R~.nlc~ug of Distributions / 12.6 A Brief Survey of Other Results
Index

Models of Computation
Ezploring the Power of Computing
John E. Savage
Brown University
Addison Wesley Longman, 1998
ISBN: 0-201-89539-0

Preface (Abridged)
Theoretical computer science treats any computational subject for which a good model can be
created. Research on formal models of computation was initiated in the 1930s and 1940s by
Taring, Post, Kleene, Church, and others. In the 1950s and 1960s programming languages, language
translators, and operating systems were under development and therefore became the subject and
53

basis for a great deal of theoretical work. The power of computers of this period was lim.ited by slow
processors and small amounts of memory, and thus theories (models, algorithms, and analysis) were
developed to explore the efficient use of computers as well as the inherent complexity of problems.
The former subject is known today as algorithm~ and data structures, the latter computational
complexity.
The focus of theoretical computer scientists in the 1960s on languages is reflected in the f~rst
textbook on the subject, Formal Langu~gea and Their Relation ~o Automata by John Hopcroft and
Jeffrey Ullman. This influential book led to the creation of many language-centered theoretical
computer science sources; many introductory theory courses today continue to reflect the content
of this book and the interests of theoreticians of the 1960s and early 1970s.
Although the 1970s and 1980s saw the development of models and m e t h o d s of analysis directed
at understandlug the limits on the performance of computers, this attractive new material has not
been made available at the introductory level. This book is designed to remedy this situation.
This book is distinguished from others on theoretical computer science by its primary focus
on real problems, its emphasis on concrete models of machines and programming styles, and the
nllmber and variety of models and styles it covers. These include the logic circuit, the Knite state
machine, the pushdown automaton, the random-access machine, m e m o r y hierarchies, the P R A M
(parallel random-access machine), the VLSI (very large-scale integrated) chip, and a variety of
parallel machines.
' -.
The book covers the traditional topics of formal languages and automata and complexity classes
but also gives an introduction to the more modern topics of space-time tradeoffs, m e m o r y hierarchies, parallel computation, the VLSI model, and circuit comPlexity. These m o d e r n topics axe
integrated throughout the text as illustrated by the early introduction of P-complet~ and N P complete problems. The book provides the first textbook treatment of space-time tradeoffs and
memory hierarchies as well as a comprehensive introduction to traditional computational complexity. Its t r e a t m e n t of circuit complexity is modern and substantive, and parallelism is integrated
throughout.

Table of Contents
I Overview of the Book

1

1 T h e R o l e o f T h e o r y in c o m p u t e r S c i e n c e

3

1.1 A Brief History of Theoretical Computer Science 4 / 1.2 Mathematical Preliminaries
7 / 1.3 Methods of Proof 14 / 1.4 Computational Models 16 / 1.5 Computational
Complexity 23 / 1.6 Parallel Computation 27 /

II G e n e r a l C o m p u t a t i o n a l
2 Logic Circuits

Models

33

36

2.1 Designing Circuits 36 / 2.2 Straight-Line Programs and Circuits 3 6 / 2 . 3 NormalForm Expausions of Boolean Functions 42 / 2.4 Reductions Between Functions 46 / 2.5
Specialized Circuits 47 / 2.6 Prefix Computations 55 / 2.7 Addition 58 / 2.8 Subtraction
61 / 2.9 Multiplication 62 / 2.10 Reciprocal and Division 68 / 2.11 Symmetric Functions
74 / 2.12 Most Boolean Functions Are Complex 77 / 2.13 Upper bounds on Circuit
Size 79
54

3 Machines with Memory

91

3.1 Finite State Machines 92 / 3.2 Simulating F S M s with Shallow Circuits 100 / 3.3
Design;-g Sequential Circuits 106 / 3.4 Random-Access Machines 110 / 3.5 RandomAccess M e m o r y Design 115 / 3.6 Computational Inequalities for the R A M 117 / 3.7
Turing Machines 118 / 3.8 Universality of the Taring Machine 121 / 3.9 'raring Machine
Circuit Simulations 124 / 3.10 Design of a Simple C P U 137
4 Finite-State Machines and Pushdown Automata

153

4.1 Finite-State Machine Models 154 / 4.2 Equivalence of DFSMs and NFSMs 156
/ 4.3 Regular Expressions 158 / 4.4 Regular Expressions and FSMs 160 / 4.5 The
P11mping Lemma for FSMs 168 / 4.6 Properties of Regular Languages 170 / 4.7 State
Minimization 171 / 4.8 Pushdown Automata 177 / 4.9 Formal Languages 181 / 4.10
Regular Language Recognition 184 / 4.11 Parsing Context-Free Languages 186 / 4.12
CFL Acceptance with Pushdown Automata 192 / 4.13 Properties of Context-Free
Languages 197
5 Computability

209

5.1 The Standard Turing Machine Model 210 / 5.2 Extensions to the Standard Tu.ring
Machine Model 213 / 5.3 Configuration Graphs 218 / 5.4 Phrase-Structure Languages
and Tu.ring Machines 219 / 5.5 Universal Tu.ring Machines 220 / 5.6 Encodings of
Strings and Tu.ring Machines 222 / 5.7 Limits on Language Acceptance 223 / 5.8
Reducibility and Unsolvability 226 / 5.9 Functions Computed by Tu.ring Machines 230

6 A l g e b r a i c a n d C o m b i n a t o r i a l Circuits

237

6.1 Straight-Line Programs 238 / 6.2 Mathematical Preliminaries 239 / 6.3 Matrix
Multiplication 244 / 6.4 Transitive Closure 248 / 6.5 Matrix Inversion 252 / 6.6 Solving
Linear Systems 262 / 6.7 Convolution ant the FFT Algorithm 263 / 6.8 Merging and
Sorting Networks 270
7 Parallel Computation

281

7.1 ParMlel Computation Models 282 / 7.2 Memoryless Parallel Computers 282 / 7.3
ParMlel Computers with Memory 283 / 7.4 The Performance of Parallel Algorithms
289 / 7.5 Multidimensional Meshes 292 / 7.6 Hypercube-Based Machines 298 / 7.7
Normal Algorithms 301 / 7.8 Routing in Networks 309 / 7.9 The PRAM Model 311 /
7.10 The BSP and LogP Models 317

325

III Computational Complexity
8 Complexity Classes

327

55

8.1 Introduction 328 / 8.2 Languages and Problems 328 / 8.3 Resource Bounds 330
/ 8.4 Serial Computational Models 331 / 8.5 Classificationof Decision Problems 334
/ 8.6 Complements of Complexity Classes 343 / 8.7 Reductions 349 / 8-8 Hard and
Complete Problems 350 / 8.9 P-Complete Problems 352 / 8.10 NP-Complete Problems
355 / 8.11 The Boundary Between P and N P 363 / 8.12 PSPACE-Complete Problems
365 / 8.13 The Circuit Model of Computation 372 / 8.14 The Parallel Random-Access
Machine Model 376 / 8.15 Circuit 'Complexity Classes 380
9 Circuit Complexity

391

9.1 Circuit Models and Measures 392 / 9.2 Relationships Among Complexity Measures
394 / 9.3 Lower-Bound Methods for General Circuits 399 / 9.4 Lower-Bound Methods
for Formula Size 404 / 9.5 The Power of Negation 409 / 9.6 Lower-Bound Methods for
Monotone Circuits 412 / 9.7 Circuit Depth 436
I0 Space-Time

Tr-adeoffs

461

10.1 The Pebble Game 462 / 10.3 Space Lower Bounds 464 / 10.4 Grigoriev's LowerBound Method 468 / 10.5 Applications of Grigoriev's Method 47'2 / 10.6 Worst-Case
Tradeoffs for Pebble Games 482 / 10.7 Upper Bounds on Space 483 / 10.8 Lower Bound
on Space for General Graphs 484 / 10.9 Branching Programs 488 / 10.10 Straight-Line
Versus Branching Programs 495 / 10.11 The Borodin-Cook Lower-Bound Method 497
/ 10.12 Properties of "nice" and "ok" Matrices 501 / 10.13 Applications of the BorodinCook Method 504
II Memory-Hierarchy

'l~radeoffs

529

11.1 The Red-Blue Pebble Game 530 / 11.2 The Memory-Hierarchy Pebble Game 533
/ 11.3 I/O-Time Relationships 535 / 11.4 The Hong-Kung Lower-Bound Method 537
11.5 Tradeofl~s Between Space and I/O Time 539 11.6 Block I / O m the MHG 555 /
11.7 Simulating a Fast Memory in the MHG 558 / 11.8 B.AM-Based I / O Models 559 /
11.9 The Hierarchical Memory Model 563 / 11.10 Competitive Memory Management
567

12 V L S I M o d e l s

of Computation

575

12.1 The VLSI Challenge 576 / 12.2 VLSI Physical Models 57'8 / 12.3 VLSI Computational Models 579 / 12.4 VLSI Performance Criteria 580 / 12.5 Chip Layout 581 /
12.6 Area-Time Tradeoffs 58fi / 12.7 The Performance of VLSI Algoritbm~ 592 / 12.8
Area Bounds 597

56

The Theory of Computation
B e r n a r d M. M o t e t
U n i v e r s i t y of N e w Mexico
A d d i s o n Wesley L o n g m a n , 1998
ISBN 0-201-25828-5

Preface
T h e o r e t i c a l c o m p u t e r science covers a wide range of topics, b u t n o n e is as f u n d a m e n t a l a n d as
useful as t h e t h e o r y of c o m p u t a t i o n . G i v e n t h a t c o m p u t i n g is o u r field of e n d e a v o r , t h e m o s t basic
q u e s t i o n t h a t we c a n ask is surely " W h a t c a n be achieved t h r o u g h c o m p u t i n g ? " ""
I n o r d e r to answer such a question, we m u s t b e g i n by defining c o m p u t a t i o n , a t a s k t h a t was
s t a r t e d last c e n t u r y by m a t h e m a t i c i a n s a n d r e m a i n s v e r y m u c h a w o r k in progress at this d a t e .
M o s t t h e o r e t i c i a n s w o u l d at least agree t h a t c o m p u t a t i o n m e a n s solving p r o b l e m s t h r o u g h t h e
m e c h a n i c a l , p r e p r o g r a m m e d e x e c u t i o n of a series of small, u n a m b i g u o u s steps. F r o m basic philosophical ideas a b o u t c o m p u t i n g , we m u s t progress to t h e definition of a m o d e l of c o m p u t a t i o n ,
formalizing t h e s e basic ideas a n d p r o v i d i n g a f r a m e w o r k in w h i c h to r e a s o n a b o u t c o m p u t a t i o n .
T h e m o d e l m u s t b e a fl'amework in w h i c h to r e a s o n a b o u t c o m p u t a t i o n . T h e m o d e l m u s t be b o t h
r e a s o n a b l y realistic (it c a n n o t d e p a r t t o o far fi'om w h a t is p e r c e i v e d as a c o m p u t e r n o w a d a y s )
a n d as u n i v e r s a l a n d p o w e r f u l as possible. W i t h a r e a s o n a b l e m o d e l in h a n d , we m a y p r o c e e d to
p o s i n g a n d resolving f u n d a m e n t a l q u e s t i o n s s u c h as " W h a t c a n a n d c a n n o t be c o m p u t e d ? " " a n d
"How efBciently c a n s o m e t h i n g be c o m p u t e d ? " "" T h e first q u e s t i o n is at t h e h e a r t of t h e t h e o r y of
c o m p u t a b i l i t y a n d t h e s e c o n d is at t h e h e a r t of t h e t h e o r y of complexity.
I n this t e x t , I have c h o s e n to give p r i d e of place to t h e t h e o r y of complexity. M y basic r e a s o n is
very simple: c o m p l e x i t y is w h a t really defines t h e limits of c o m p u t a t i o n . C o m p u t a b i l i t y establishes
s o m e a b s o l u t e limits, b u t limits t h a t do n o t t a k e into a c c o u n t a n y resource usage are h a r d l y limits in
a p r a c t i c a l sense. M a n y of t o d a y " s i m p o r t a n t p r a c t i c a l q u e s t i o n s in c o m p u t i n g are b a s e d o n resource
p r o b l e m s . For i n s t a n c e , e n c r y p t i o n of t r a n s a c t i o n s for t r a n s m i s s i o n s over a n e t w o r k c a n n e v e r
b e entirely p r o o f a g a i n s t s n o o p e r s , b e c a u s e an e n c r y p t e d t r a n s a c t i o n m u s t be d e c r y p t e d by s o m e
m e a n s a n d t h u s c a n always be d e c i p h e r e d by s o m e o n e d e t e r m i n e d to do so, given s,,fBcient resources.
However, t h e real goal of e n c r y p t i o n is to m a k e it sufficiently "hard" " " - - t h a t is, sufficiently resourcei n t e n s i v e to d e c i p h e r t h e m e s s a g e t h a t s n o o p e r s will b e d i s c o u r a g e d or t h a t even d e t e r m i n e d
spies will t a k e t o o long to c o m p l e t e t h e d e c r y p t i o n . I n o t h e r words, a g o o d e n c r y p t i o n s c h e m e
does n o t m a k e it i m p o s s i b l e to d e c o d e t h e message, j u s t very d i f B c u l t - - t h e p r o b l e m is n o t one of
c o m p u t a b i l i t y b u t one of complexity. As a n o t h e r e x a m p l e , m a n y t a s k s c a r r i e d o u t b y c o m p u t e r s
t o d a y involve s o m e t y p e of o p t i m i z a t i o n : r o u t i n g of planes in t h e sky or of p a c k e t s t h r o u g h a
n e t w o r k so as to get p l a n e s or packets to their d e s t i n a t i o n as efficiently as possible; a l l o c a t i o n of
m a n u f a c t u r e d p r o d u c t s to w a r e h o u s e s in a retail chain so as to rninimi~.e w a s t e a n d f u r t h e r s h i p p i n g ;
p r o c e s s i n g of raw m a t e r i a l s into c o m p o n e n t p a r t s (e.g., c u t t i n g c l o t h into p a t t e r n s pieces or cracking
c r u d e oil into a r a n g e of oils a n d distillates) so as to m i n i m i z e waste; d e s i g n i n g n e w p r o d u c t s to
m l n l m i z e p r o d u c t i o n costs for a given level of p e r f o r m a n c e ; a n d so forth. All of t h e s e p r o b l e m s are
c e r t a i n l y c o m p u t a b l e : t h a t is, each such p r o b l e m has a well-defined o p t i m a l s o l u t i o n t h a t c o u l d be
f o u n d t h r o u g h s , , ~ c i e n t c o m p u t a t i o n (even if this c o m p u t a t i o n is n o t h i n g m o r e t h a n a n e x h a u s t i v e
search t h r o u g h all possible solutions). Yet t h e s e p r o b l e m s are so c o m p l e x t h a t t h e y c a n n o t be
solved o p t i m a l l y w i t h i n a r e a s o n a b l e a m o u n t Of time; indeed, even d e r i v i n g g o o d a p p r o x i m a t e
s o l u t i o n s for t h e s e p r o b l e m s r e m a i n s resource-intensive. T h u s t h e c o m p l e x i t y of solving ( e x a c t l y
57

or approximately) problems is what determines the usefnlness of c o m p u t a t i o n in practice. It is no
accident t h a t complexity theory is the most active area of research in theoretical c o m p u t e r science
today.
Yet this text is not just a text on the theory of complexity. I have two reasons for covering
additional material: one is to provide a graduated approach to the often challenging results of
complexity theory and the other is to paint a suitable backdrop for the unfolding of these results.
The backdrop is mostly computability theory--clearly, there is little use in aslring w h a t is the complexity of a problem t h a t cannot be solved at all! The g r a d u a t e d approach is provided by a review
chapter and a chapter on fi.nite automata. Finite a u t o m a t a should already be s o m e w h a t familiar
to the reader; t h e y provide an ideal testing ground for th e ideas and m e t h o d s need in working
with complexity models. On the other hand, I have deliberately o m i t t e d theoretical topics (such as
formal grammars, the Chomsky hierarchy, formal semantics, and formal specifications) t h a t , while
interesting in their own right, have limited impact on everyday computing ~ome because t h e y are
not concerned with resources, some because the models used are not well accepted, and g r a m m a r s
because their use in compilers is quite different fTom their theoretical expression in the Chomsky
hierarchy. Finite a u t o m a t a and regular expressions (the lowest level of the ChomRky hierarchy) are
covered here but only by way of an introduction to (and contrast with) the u.uiversal models of
comput a t i on used in computability and complexity.
â¢Of course, not all results in the theory of complexity have the same impact on computing,
Like any rich b o d y of theory, complexity theory has applied aspects and very abstract ones. I
have focused on the applied aspects: for instance, I devote an entire chapter on how to prove
t h a t a problem is h a r d but less t h a n a section on the entire topic of structure t h e o r y (the p a r t of
complexity theory t h a t addresses the internal logic of the field). Abstract results found in this text
are mostly in support of fundamental results t h a t are later exploited for practical reasons.
Since theoretical computer science is often the most challenging topic studied in the course of a
degree program in computing, I have avoided the dense presentation often favored by theoreticians
(de~n;tions, theorems, proofs, with as little text in between as possible). Instead, I provide intuitive
as well as formal support for further derivations and present the idea behind any line of reasoning
before formalizing said reason;ug. I have included large plumbers of examples and illustrated m a n y
abstract ideas t hr ough diagrams; the reader will also find useful synopses of m e t h o d s (such as
steps in an NP-completeness proof) for quick reference. Moreover, this text offers strong support
thro ugh t he Web for b o t h students and instructors. Instructors will find solutions for most of t h e
250 problems in the text, along with m a n y more solved problems; students will find interactive
solutions for chosen problems, testing and validating their reasoning process along th e way r a t h e r
t h a n delivering a complete solution at once. In addition, I will also acc~,mulate on th e Web site
addenda, errata, co~nrnents form students and instructors, and pointers to useful resources, as well
as feedback mechani~rnR--I want to hear fi'om all users of this text suggestions on how to improve it.
The Ulq.L for t he Website is h t t p : / / t ~ m , cs .,,,~.edu/~moret/computal:ion/; m y email address
is motet@ca, l m . . ode.
Table
1. I n t r o d u c t i o n

of Contents

1

i . I Motivation a nd Overview 1 / 1.2 History 5

2. P r e l i m i n a r i e s

11

58

2.1 Nnmbers and Their Representation 11 / 2.2 Problems, Instances, and Solutions 12
/ 2.3 Asymptotic Notation 17 / 2.4 Graphs 20 / 2.5 Alphabets, Strings, and Languages
25 / 2.6 Functions and l~Â¢_uiteSets 277 / 2.7 Pairing Functions 31 / 2.8 Cantor" s Proof:
The Technique of Diagonalization 33 / 2.9 Implications for Computability 35 / 2.10
Exercises 37 / 2.11 Bibliography 42
3. F i n i t e A u t o m a t a

and Regular Languages

43

3.1 Introduction 43 / 3.2 Properties of Finite Automata 54 / 3.3 Regular Expressions
59 / 3.4 The Pumping LemmR and Closure Properties 770 / 3.5 Conclusion 85 / 3.6
Exercises 86 / 3.7 Bibliography 92
4. U n i v e r s a l M o d e l s o f C o m p u t a t i o n

93

4.1 Encoding Instances 94 / 4.2 Choosing a Model of Computation 97 / 4.3 Model
Independence 113 / 4.4 Turing Machines as Acceptors and Ewlmerators 115 / 4.5
Exercises 117 / 4.6 Bibliography 120
5. C o m p u t a b i l i t y T h e o r y

121

5.1 Primitive Recursive Functions 122 / 5.2 Partial Recursive Functions 134 / 5.3
Arithmetization: Encoding Taring Machine 137 / 5.4 ProgrAmming Systems 144 / 5.5
Recursive and R. E. Sets 148 / 5.6 Rice's Theorem and the Recursion Theorem 155 /
5.77Degrees of Unsolvability 159 / 5.8 Exercises 164 / 5.9 Bibliography 1677
. Complexity Theory: Foundations

169

6.1 Reductions 1770 / 6.2 Classes of Complexity 1778 / 6.3 Complete Problems 200 / 6.4
Exercises 219 / 6.5 Bibliography 223
7,. P r o v i n g P r o b l e m s H a r d

225

7.1 Some Important NP-Complete Problems 226 / 7.2 Some P-Completeness Proofs
253 / 7.3 From Decision to Optimization and Enumeration 260 / 77.4 Exercises 275 /
7.5 Bibliography 284
8. C o m p l e x i t y T h e o r y in P r a c t i c e

285

8.1 Circumscribing Hard Problems 286 / 8.2 Strong NP-Completeness 301 / 8.3 The
Complexity of Approximation 308 / 8.4 The Power of Randomization 335 / 8.5 Exercises 346 / 8.6 Bibliography 353
9. C o m p l e x i t y T h e o r y : T h e F r o n t i e r

357,
59

9.1 Introduction 357 / 9.2 The Complexity of Specific Instances 360 / 9.3 Average-Case
Complexity 367 / 9.4 Parallelism and Commlm|cation 372 / 9.5 Interactive Proofs and
Probabilistic Proof Checking 385 / 9.6 Complexity and Constructive Mathematics 396
/ 9.7' Bibliography 403

8. R e f e r e n c e s
A. P r o o f s

407'
421

A.1. Quod Erat Demonstrandum, or W h a t is a Proof? 421 / A.2. Proof Elements 424
/ A.3. Proof Terhniques 425 / A.4. How to Write a Proof 437 / A.5. Practice 439

60

